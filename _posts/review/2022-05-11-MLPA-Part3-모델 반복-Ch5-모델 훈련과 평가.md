---
layout: post
title:  "[MLPA] Part3/Ch.5 - 모델 훈련과 평가 "
excerpt: "Building Machine LEarning Powered Applications - Part3 - Ch.5 정리"

categories:
  - review
tags:
  - [Books]

toc: true
toc_sticky: true
 
date: 2022-05-11
last_modified_at: 2022-05-17

---

* 해당 블로그는 책 '머신러닝 파워드 애플리케이션 (한빛미디어)' 을 읽고 개인적으로 정리한 글입니다.

* 번역서 코드 주소: https://github.com/rickiepark/ml-powered-applications

#### 머신러닝 전체 과정
Part 1 - 올바른 머신러닝 접근 방법 모색
Part 2 - 초기 프로토타입 제작
**Part 3 - 모델 반복**
Part 4 - 배포와 모니터링

# Part 3 - 모델 반복
* 머신러닝은 실험이 필수이기 때문에 과정이 매우 반복적
* 모델 반복: 분석 (성능 병목 탐색) > 접근 방법 선택 (실험 목록 생성) > 구현 (모델, 파이프라인 구현) > 측정 (독자적인 대시보드 생성)> (반복) > 분석...
* Ch.5 모델 훈련과 평가: 초기 모델을 훈련하고 성능을 평가. 그 다음 성능을 자세히 분석하여 개선하는 방법 소개
* Ch.6 머신러닝 문제 디버깅: 모델을 빠르게 구축 / 디버깅 기법 / 시간을 낭비하게 만드는 오류를 피하는 방법 소개
* Ch.7 분류기를 사용한 글쓰기 추천: 예시 제공. 예시 - 추천 모델. 훈련된 분류기를 사용해 사용자에게 추천을 제공하는 방법 소개

---

## Ch.5 모델 훈련과 평가
### Summary (+ 5.4. 마치며)
* (이전 장) Part 2 요약: 문제 정의, 문제 해결 계획 세우기, 데이터셋 탐색, 초기 특성 생성 --> 적절한 모델을 훈련하기 위한 충분한 정보 획득
* Ch.5. 요약
    * 5.1. 모델 선택 시 고려사항 & 실제 조건에서 모델 평가하는데 도움이 되도록 데이터를 분할하는 모범 사례 설명
        * (해당 장에서 설명하는) 적절한 모델이란? 현재 작업에 적합하고 좋은 성능을 낼 가능성이 높은 모델 
    * 5.2. & 5.3. 모델의 결과를 분석하고 오차를 진단하는 방법 설명
* 5.1. 가장 간단하고 적절한 모델
    * 초기 모델을 결정하는 기준
    * 데이터셋 분할의 중요성과 데이터 누수를 피하기 위한 방법
* 5.2. 모델 평가: 정확도를 넘어서
    * 초기 모델 훈련 후 모델의 예측과 데이터를 비교하고 대조하는 여러가지 방법을 통해 모델이 얼마나 잘 동작하는지 분석
* 5.3. 특성 중요도 평가
    * 모델 자체를 조사함으로써 모델이 예측에 사용한 특성에 대한 직관을 얻음

### 5.1. 가장 간단하고 적절한 모델
* 가능한 모든 모델로 시도 및 TEST dataset 의 특정 측정 지표 기준으로 결과를 평가하는 경우 --> 일반적으로 이 방법이 최선이 아님
    * 계산 비용이 높음
    * 모델을 블랙박스 처럼 다룸
    * **머신러닝 모델이 학습하는 과정에서 암묵적으로 데이터에 대해 어떤 가정을 한다는 것**을 완전히 무시 
* 모델마다 데이터에 대한 가정이 다르므로 적합한 작업이 따로 존재

#### 5.1.1. 간단한 모델
* 간단한 모델이란? 1) 빠른 구현 2) 쉬운 이해 3) 배포 가능
* 단순성 비교를 위해 주어진 문제에 맞는 자신만의 비교 표를 만드는 것도 좋음
* 비교 표 항목 예시
    * 1) 구현의 용이성
        * 1-1) 이해하기 쉬운 모델
        * 1-2) 검증된 구현
    * 2) 해석 능력
        * 2-1) 특성 중요도 추출 용이
        * 2-2) 디버깅하기 쉬움
    * 3) 배포 능력
        * 3-1) 추론 시간
        * 3-2) 훈련 시간

##### 1) 빠른 구현 (구현의 용이성)
* 구현이 간단한 모델: 이해하기 쉽고, 튜토리얼에 많이 사용되고, 도움을 얻을 수 있는 모델
* e.g. Keras, Scikit-learn 과 같이 이미 잘 알려진 라이브러리에 있는 모델 사용

##### 2) 쉬운 이해 (해석 능력)
* 모델의 설명 가능성 explainability & 해석 능력 interpretability 란? 예측을 만들게 된 이유 제시 (e.g. 특성 조합 like feature importance)
* 모델의 Explainability 를 통해
    * 모델 편향 확인
    * 예측 결과 향상 방향성 확인
    * 반복과 디버깅 용이
* 모델의 Interpretability 를 통해
    * 더 나은 선택을 하도록 특성 추가, 변경, 제거 방향성 확인 

##### 3) 배포 가능 (배포 능력)
* 모델의 최종 목표는 **사용자에게 가치 있는 서비스를 제공하는 것**
    * 예측 속도: 훈련된 모델이 사용자에게 예측을 제공하는데 시간이 얼마나 소요되는가?  
        * 모델 결과 출력 소요 시간 + 사용자가 요청을 보내고 결과를 받는 과정 소요 시간
        * 변수 전처리, 네트워크 전송, 모델 출력, 사용자에게 보여질 데이터 사이에 필요한 후처리 단계 포함
    * 예상하는 동시 접속자 수 고려: 동시 접속자 수 고려 시 추론 파이프라인이 충분히 빠른지
    * 모델 훈련 시간: 모델 훈련하는데 소요되는 시간, 얼마나 자주 훈련해야하는지 등

#### 5.1.2. 패턴에서 모델로
* 데이터 내 패턴에 따른 적절한 모델 예시

##### 1) Feature (특성) 들 간 Scale 차이가 큰 경우
* 방법 1) 정규화
* 방법 2) 특성 스케일의 차이에 영향을 받지 않는 모델 선택
    * e.g. Tree 계열: Decision Tree, Random Forest, Gradient Boosted Decision Tree, XGBoost
    *  _XGBoost: 그래디언트 부스팅 트리 타입 중 하나로 안정성이 높고 속도가 빨라 실전에서 널리 사용됩니다_

##### 2) Target (Y) 이 Features (Xs) 의 선형 조합인 경우
* 특성의 선형 조합만 사용해 좋은 예측이 가능하다고 판단 되는 경우
* 회귀문제: Linear Regression / 분류문제: Logistic Regression, Naive-Bayes Classification 활용 가능
    * 간단하고 효율적, 종종 모델의 가중치를 직접 보고 중요 특성 식별 가능
* 특성과 타깃간 관계가 복잡한 경우: 다중 신경망 같은 비선형 모델 사용 OR 교차 특성 Intersection Feature 생성 가능

##### 3) 데이터 내 시계열 특징이 있는 경우
* 시계열 정보를 명시적으로 인코딩할 수 있는 모델 사용
    * e.g. ARIMA (AutoRegressive Integrated Moving Average; 자기회귀누적이동평균), RNN (Recurrent Neural Network; 순환 신경망)

##### 4) 각 Data Sample 이 특정 Pattern의 조합으로 이루어져 있는 경우 (e.g. 이미지 분야)
* 각 데이터 샘플이 국부적인 패턴을 포함한다고 판단 되는 경우
    * e.g. 이미지, 음성 인식, 텍스트 분류 
* CNN (Convolutional Neural Network; 합성곱 신경망) 활용 가능
    * CNN: 이동불변성 (translation invariant) 필터를 학습

#### 5.1.3. 데이터셋 분할
* 모델의 핵심 목표는 **사용자의 입력 데이터에 대한 합리적인 예측을 제공하는 것** = **이전에 본적 없는 데이터에서 잘 동작 해야한다**
* 모델이 너무 잘 동작하는 경우, 버그나 데이터 누수 의심
    * 머신러닝의 머피의 법칙: 테스트 데이터에서 모델이 놀라운 성능을 낼수록 모델 파이프라인에 오류가 있을 가능성이 높다 

##### 데이터 분할
* 일반적으로 Train: Valid: Test set 분할
    * Validation Set: 모델이 본 적 없는 데이터에 일반화 generalization 할 수 있는지 검증
        * 교차 검증 cross-validation: train:valid 분할 후 모델을 훈련하는 괒어을 여러번 반복. 이 경우 검증 세트에 따른 평가 점수의 변동 제어 가능
    * Test set: 모델이 validation set에 편향되는 것을 방지하기 위해 추가로 활용.    
    * **모델링 결정을 내리는 데 테스트 셋의 성능을 사용하지 않는 것이 중요**
        * Why? 과대평가 위험. 테스트 셋의 경우 실전에 투입했을 때 본 적 없는 데이터를 대표하기 때문
 
 ##### 데이터 누수 Data Leakage
 * 데이터 누수는 실전에서 사용자가 사용할 때 얻을 수 없는 정보를 훈련 과정에서 모델이 얻을 때 일어남
 * 1) 시계열 데이터 누수
    * 미래 이벤트를 예측하는 데이터셋에서 --> 훈련하는 모델이 '미래 데이터'를 사용하는 경우
 * 2) 샘플 오염
    * 중복 샘플을 가진 데이터셋에서 --> 훈련할 때 동일 샘플이 검증 세트와 테스트 세트에도 포함되는 경우
    * 한 샘플을 여러 번 측정한 데이터셋에서 훈련할 때 -->. 학습에 사용한 샘플의 다른 측정값이 검증 세트에 포함되는 경우
* 데이터 누수를 피하는 예시
    *  문제 상황: 학생의 수필 점수 예측
    *  랜덤 샘플링하는 경우 문제: 많은 학생들이 여러 개의 수필을 쓰는 썼기 때문에, 동일한 학생의 수필이 훈련 셋과 테스트 셋 동시에 들어갈 수 있음 --> 이전에 본 적없는 학생의 점수는 잘 예측하지 못할 수 있다
    *  해결 방법: 데이터 분할 시 수필 단위가 아닌 학생 단위로 분할

#### 5.1.5. 성능 평가
* 훈련 세트와 검증 세트에 대한 비용 함수 값을 비교함으로써 편향-분산 트레이드오프 (bias-variance tradeoff) 추정 가능
* 클래스 불균형이 심한 경우: Precision, Recall, F1-score 활용 가능

##### 편향-분산 트레이드오프
* 편향-분산 트레이드 오프란? 모델의 복잡도가 증가함에 따라 오차의 형태가 어떻게 바뀌는지를 나타내는 것
    * 보통 모델의 복잡도가 올라갈수록, 분산은 증가하고 편향을 줄어든다. --> 따라서 모델의 복잡도 기준 양 극단에서 오차의 합 (분산 + 편향) 이 증가하게 된다.
* 과소적합과 과대적합은 편향-분산 트레이드 오프의 양 극단의 경우이다.
* 편향-분산 트레이드오프와 싸우려면 훈련 세트에서 모델의 성능을 높여 편향을 줄이는 것과 검증 세트의 성능을 높여 (이로 인해 훈련 세트의 성능이 나빠질 수도 있다) 분산을 줄이는 것 사이에서 최적의 지점을 찾아야 한다.

##### 성능지표 도출 이후
* 성능 검증
    * 성능 지표만으로 평가하면 오해를 일으킬 수 있다 (e.g. 클래스 불균형 문제에서 전체 정확도로 평가하는 경우) 
* 반복
    * 모델 구축은 반복적인 과정. 반복 루프를 시작하는 가장 좋은 방법은 개선할 점과 개선 방법을 찾는 것 
    * 성공적인 모델을 빠르게 구축하는 핵심 열쇠 **모델이 실패하는 특정 이유를 찾아 해결하는 것**
    * 모델이 어려워하는 곳과 파이프라인에서 개선이 필요한 부분을 식별하는 데 성능 지표는 큰 도움이 되지 않음

### 5.2. 모델 평가: 정확도를 넘어서 (부제: 모델 심층 평가)
* 모델이 데이터를 표현할 능력이 있는지 --> 방법 1) 모델의 예측과 레이블을 대조하여 성능을 조사
* 현재 데이터셋이 충분히 균형 잡힌 데이터셋인지
* 현재 데이터셋이 충분히 대표성이 있는 sample을 포함하고 있는지 여부 확인
 
#### 5.2.1. 데이터와 예측 대조하기 
* 하나의 수치로 요약하는 지표 대신에, 데이터의 부분집합을 사용해 정확도, 정밀도, 재현율 같은 성능지표를 따로 계산해서 비교

#### 5.2.2. 오차 행렬 Confusion matrix
* 클래스 별 True vs Prediction 값 비교 가능
* 클래스 수가 많거나 불균형한 데이터셋에 유용
![image](https://user-images.githubusercontent.com/98376833/168807709-26e5843b-dc52-4fc1-8108-498f85dbb5b8.png)
    * 출처: Punn, N. S., & Agarwal, S. (2021). Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray images using fine-tuned deep neural networks. Applied Intelligence, 51(5), 2689-2702.
    * Recall = class-wise (positive) accuracy = Sensitivity

#### 5.2.3. ROC 곡선 (ROC; Receiver Operating Characteristic; 수신자 조작 특성) 
* ROC 곡선이란? 거짓 양성 비율 (FPR; False Positive Rate) 의 함수로 진짜 양성 비율 (TPR; True Positive Rate)를 그린 것.
    * TPR = Recall = class-wise (positive) accuracy = Sensitivity  =  TP / (TP+FN) = positive로 잘 예측 / 전체 positive samples
    * FPR = FP / (FP + TN) = positive 로 잘못 예측 / 전체 negative samples
* 결정임계값 (Decision Threshold) 를 0~1 사이에서 규칙적으로 바꾸면서 각 지점에서 TPR과 FPR을 계산하면 ROC 곡선을 얻을 수 있음
    * 결정임계값: 대부분의 분류 모델은 sample 별로 특정 클래스에 속할 확률 점수를 반환. 추론 시 모델이 만든 확률이 어떤 임계값 이상이면, 샘플을 특정 클래스에 할당할 수 있다는 의미를 가짐. 이 때의 어떤 임계값 (특정 클래스 할당 기준) = 결정임계값. 
    * 대부분의 분류기는 50% 의 확률을 결정 임계값으로 사용하나, 문제에 따라 변화시킬 수 있음.
![image](https://user-images.githubusercontent.com/98376833/168809921-02c91301-ee0f-47ba-a24b-b4ba6b94a265.png)
    * 출처: https://en.wikipedia.org/wiki/Receiver_operating_characteristic    
* ROC 곡선의 이해
    * 왼쪽 아래에서 오른쪽 위로 이어진 대각선 = 무작위 예측
    * 어떤 FPR 구간에도  TPR = 1 에 가까울수록 = 완벽한 모델
* AUC (Area Under the Curve) : ROC 곡선의 아래 면적
    * AUC = 0.5 --> 랜덤한 모델
    * AUC = 1.0 --> 완벽한 모델
* **하지만 실제 애플리케이션에서는 해당 문제에서 가장 유용한 TPR/FPR 비율을 만드는 decision threshold 선택이 필요**
    * **ROC 곡선에 수직선과 수평선을 그어 제품의 요구사항을 나타내면, 단순히 가장 높은 AUC 점수를 구하는 것보다 더 구체적인 목표를 가질 수 있다**
    * 예시) FPR (실제 Negative 를 Positive로 잘못 예측) 한도를 10%로 설정하는 경우, FPR 10% 미만 기준에서 찾을 수 있는 최선의 모델  사용 필요 (e.g. FPR 10% 미만에서의 AUC 값)
* **ROC 곡선은 모델의 예측을 더 보수적으로 혹은 덜 보수적으로 만드냐에 따라 성능이 어떻게 변화하는지 자세한 정보 제공이 가능**

#### 5.2.4.보정 곡선 (Calibration Curve) (= also known as _reliability diagrams_)
* 보정곡선이란? 
    * 분류기의 신뢰도 (reliability) 에 대한 함수. 진짜 양성 샘플의 비율을 나타냄
    * X축: mean predicted probability (평균예측값) / Y축: fraction of positives (양성 샘플의 비율)
    * 모델 예측 확률을 살펴보는 방법으로 **분류문제**에서 사용 가능
    * 예측 분포를 진짜 클래스 분포와 비교하여 보정 calibration 이 잘되었는지 확인  
    * 양성 샘플의 비율 , 평균 예측값 = scikit-learn calibration_curve()
* 보정곡선과 예측값 히스토그램 (평균예측값 별 양성샘플의 count(빈도수)) 을 통해 모델의 출력 확률을 신뢰할 수 있는지 가늠
* 보정 곡선의 이해
    * 왼쪽 아래에서 오른쪽 위로 이어진 대각선 = 완벽한 모델
    * Scikit-learn Calibation curve: https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html
![image](https://user-images.githubusercontent.com/98376833/168813129-3fff7e77-2dd1-4e6b-9de4-7c3e00a4a5d8.png)
* Brier Score (브라이어 점수): 예측 확률과 정답의 차이를 제곱하여 평균낸 것 (0이 최고, 1은 최악) 
    * scikit-learn brier_score_loss()

#### 5.2.5. 오차를 위한 차원 축소
* 차원 축소 기법을 통해 오차 분석 수행 --> 오차가 밀집된 영역을 찾음
* 차원 축소를 통해 시각화 한 후, 데이터 포인터 별로 모델 성능에 따라 색깔을 달리하여 오차를 구별하거나
* 데이터에 군집 알고리즘을 적용한 후, 각 클러스터에서 모델의 성능을 측정하고 가장 성능이 나쁜 클러스터 확인. --> 이 클러스터 안의 샘플을 조사하여 필요한 특성 생성 가능 

#### 5.2.6. Top-k 방법
* Train/Valid Set 에서 각각 top-k 메서드를 사용하는 경우, training 과정과 현재 데이터셋에 대한 한계 파악에 용이
* 5.2.5. 오차를 위한 차원 축소 vs 5.2.6. Top-k 방법 
    * 5.2.5. 오차를 위한 차원 축소: 모델의 실패 종류를 파악하기 위해 차원 축소를 사용해 오차가 밀집된 영역을 찾음
    * 5.2.6. Top-k 방법: 모델 자체를 사용하여 오차가 밀집된 영역을 찾음 (가장 어렵거나 불확실하게 예측하는 샘플을 찾음)
* Top-k 방법이란?
    * 모델의 신뢰도 점수 (e.g. 각 샘플의 predicted probability ) 를 활용하여 모델 파악
    * 각 클래스나 5.2.5. 에서 찾은 클러스터에 대해 아래 내용 시각화
        * 최상의 k개 샘플 : 모델이 쉬운 샘플 구별 가능
        * 최악의 k개 샘플 : 모델이 어려운 샘플 구별 가능
        * 가장 불확실한 k개 샘플 : 모델이 혼돈되는 샘플 구별 가능
    * github: def get_top_k()
##### 최상의 k개 샘플 
* 모델이 정확하게 예측하고 가장 강한 확신을 가지는 샘플 k개 출력 (e.g. largest class-wise predicted probability top-k samples)
* 목표: 모델의 성능을 설명할 수 있는 공통된 특성값을 찾음 --> 모델이 잘 활용하는 특성 파악

##### 최악의 k개 샘플
* 모델이 잘못 예측하고 가장 강한 확신을 가지는 샘플 출력 (e.g. smallest class-wise predicted probability top-k samples)
* 실패하는 샘플의 트렌드 확인 가능 --> 모델의 작업을 쉽게 만드는 **추가 특성** 찾는데 도움
* Validation set에 있는 최악의 k개 샘플을 시각화하면, Training Set 과 크게 다른 sample 확인도 가능. 
    * 검증 세트에 너무 어려운 샘플이 있는 경우 _5.1.3. 데이터셋 분할_  을참고하여 데이터 분할 전략 업데이트 하는 것도 방법

##### 가장 불확실한 k개 샘플
* 모델의 예측에 가장 확신이 적은 샘플을 출력
    * 일반적인 경우 (결정임계값 decision threshold = 0.5), 각 클래스에 대한 모델 출력 (확률)이 동일할수록 불확실한 샘플
    * (=) 결정임계값에 가까운 예측확률을 갖는 샘플
* 레이블 충돌이 있는 샘플 확인 가능
* Validation set에 있는 불확실한 top-k개 샘플을 시각화하면, Training Set에 있어야 할 데이터 종류를 알 수 있음. 
    * e.g.  모델에게는 불확실하지만 사람에게는 명확한 validation sample이 있다면, 모델이 훈련 셋트에 이런 종류의 데이터를 추가할 수 있음.

#### 5.2.7. 분류 모델 외 다른 모델
* 다른 모델에 분류 프레임워크 활용하는 방법
    * Object Detection: 예측 박스와 레이블 박스의 중첩도 (IoU 또는 Jacard Index) 바탕으로 정확도 계산
    * 다중 클래스를 갖는 경우 (e.g. 콘텐츠 추천 모델, 생성 모델), 여러 카테고리로 분할 후 각 카테고리에 대해 성능을 계산

### 5.3. 특성 중요도 평가
* 모델이 데이터를 표현할 능력이 있는지 --> 방법 2) 모델의 예측 해석 (특정 중요도 평가)
* 모델에 도움이 되지 않는 특성을 제거하거나 반복하는 데 도움이 됨
* 데이터 누수 (실전에서 얻을 수 없는 정보를 훈련과정에서 모델이 얻는 경우) 와 같이 의심스러운 예측 성능을 제공하는 특성을 찾을 때도 유용
    * 어떤 특성이 기대한 것 보다  >> 모델의 특성 중요도가 더 높게 나오는 경우, --> 훈련 세트에서 이런 특성을 가진 샘플을 조사 & 데이터셋 분할 방법 재확인 & 데이터 누수 의심
 
#### 5.3.1. 분류기 직접 활용하기 
* Linear 모델의 계수값
* Tree 모델의 feature importance

#### 5.3.2. 블랙박스 설명 도구 (a.k.a. XAI; eXplainable AI)
* 모델의 내부 동작과는 상관없이 모델의 예측을 설명하는 알고리즘 활용 
* e.g.  전체가 아닌 주어진 샘플 (단일 sample 또는 특징이 비슷한 같은 클래스의 sample group) 에서 영향력이 높은 모델의 특성을 감지하기 위해, 어떤 샘플에 대해 각각의 특성값을 바꾸면서 이로 인해 모델의 예측이 어떻게 바뀌는지 관찰하는 방법: LIME, SHAP
    * 개별 샘플에 국지적 XAI 를 적용하고 그 결과를 모으는 방법으로 전체 샘플의 trend 확인 가능
    * LIME
        * https://myeonghak.github.io/xai/XAI-LIME(Local-Interpretable-Model-agnostic-Explanation)-알고리즘/
    * SHAP
    * github: black_box_explainer.ipynb

---

