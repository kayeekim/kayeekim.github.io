---
layout: post
title:  "[DL] RNN"
excerpt: "Deep Learning Basic - RNN"

categories:
  - ds
tags:
  - [DL]

toc: true
toc_sticky: true
 
date: 2022-04-08
last_modified_at: 2022-04-08

---

## Summary 
* 시계열 분석 알고리즘 종류 (Statistic, ML, DL based)
* RNN 기본 개념

# 시계열 분석 알고리즘 종류
## Statistic based
* Moving Average
* Exponential Smoothing
* ARIMA (Autoregressive integrated moving acerage)
* SARIMA (Seasonal ARIMA)
* Binary variable model
* Trigonometric model
* Growth curve model
* Time series regression with autocorrelation

## ML based
* Support Vector Machine/Regression
* Random Forest
* Boosting
* Gaussian Process
* Hidden Markov Model

## DL based
* Recurrent neural network (RNN)
* LSTM
* GRU
* Seq2Seq, Seq2Seq with Attention
* Transformer
* GPT-1, BERT, GPT-3

# What is a Recurrent Neural Network (RNN)
* 입출력 데이터 (e.g. Sequence) 길이에 관계없이 입력과 출력을 받아들일수 있는 네트워크 구조
* Sequence Data를 처리할 때 용이

## Feedforward Neural Networks vs Recurrent NNs 
### RNN의 핵심 아이디어
* Neuron 에 시퀀스가 처리될 때 업데이트 되는 "내부 상태" 개념이 존재
* 시퀀스가 처리될 때 마다 업데이트 된다 = 데이터가 순환된다 => 과거의 정보를 기억 + 최신 데이터 반영 (update)
* 모든 time step 에서의 반복 공식
    * $ h_t = f_W(h_(t-1), x_t) $  where $h_t$: new state; state at time step t / $f_W$: some function with parameters W / $h_(t-1)$ : old state; state at time step t-1 / $x_t$: input vector at time step t
    * 모든 time step 에서 동일한 $f_W$ (function with parameters W) 가 사용 =  "activate function and weight" sharing

### Process Sequence + 예시
#### Feedforward NNs
* one to one: Image clasification (Image -> Label)

#### Recurrent NNs (순환신경망)
* one to many: Image Captioning (Image -> Sequence of words)
* many to one: Video classification (Sequence of images -> label)
* many to many
    * Machine Translation (Sequence of words -> Sequences of words)
    * Per-frame video classification (Sequence of images -> Sequence of label) 

## RNN 종류
![image](https://user-images.githubusercontent.com/98376833/162374683-9ceef1af-2156-447b-b013-858019d16ec4.png)
* (출처: [PyTorch] 시계열 데이터를 위한 RNN/LSTM/GRU 사용법과 팁)

### Vanilla RNN (Same as "Elman RNN")
* The state consists of a single "hidden" vector h :
    * $h_t = f_W(h_(t-1), x_t, bias)$
    * => (Vanilla/Elman) $h_t = tanh(W_(hh) h_(t-1) + W_(xh) x_t + bias)$ where $W_(hh)$: weight of hidden to hidden / $W_(xh)$: weight of input to hidden
        * hidden state의 activate function 으로는 주로 tanh 나 ReLU 사용
    * $y_t = W_(hy) h_t + bias$ where $W_(hy)$: weight of hidden to output
        * output의 activate function 으로는 주로 sigmoid 나 softmax 사용
    * *$h_0$: hidden state at time state 0 (초기 hidden state 값)은 보통 무작위로 초기화하거나, 0으로 초기화한다. (0으로 초기화하는 경우가 가장 일반적)

* a hidden node 구조: 표준 RNN 모델의 경우, a hidden state (모듈 하나) 내 tanh (activate) layer 한 층을 가지고 있음 
* 단점: long term 의존성 (Long-Term Dependency) 문제 - 기울기 소멸 (gradient vanishing) or 폭발 


### LSTM (Long Short-Term Memory) 
* 장점 : Vanilla RNN의 long term 의존성 (Long-Term Dependency) 문제 일부 해결
* a hidden state 구조: 단순 1개 layer 대신 4개 layer가 특별한 방식으로 서로의 정보를 주고 받는다

#### LSTM 구조 
* 총 4개 layer - Cell State 1개 + Gates 3개 (forget, input, output gate) 로 구성
* Cell state $C_t$
    * 전체 메모리 역할
    * Gate라고 불리는 구조에 의해 정보가 추가/제거
* Gates $f, i, o$ 
    * 학습을 통해 어떤 정보를 유지하고 어떤 정보를 제거할지 학습
    * LSTM은 3개의 Gate를 가지고 있으며, 이 gate 들은 cell state 를 보호하고 제어
        * forget gate (f): 과거 정보를 제거하기 위한 게이트
        * input gate (i): 현재 정보를 기억하기 위한 게이트
        * output gate (o) 최종 결과를 내보내기 위한 게이트
    * 모든 gate는 sigmoid 함수 사용 (*sigmoid 함수값 출력범위: 0~1)
        * sigmoid 값 ~ 0: 아무 정보도 넘기지 마라 / 1: 모든 정보를 넘겨라
     * input gate, output gate 의 경우 sigmoid, tanh 모두 사용

### GRU (Gated Recurrent Unit)
* GRU는 LSTM을 구성하는 Time-Step의 Cell을 조금 더 간소화한 버전
* 장점: LSTM 대비 빠른 학습속도, 성능은 LSTM과 비슷하다는 평
    * 데이터 양이 적을 때, 성능: (매개변수의 양이 더 적은) GRU > LSTM 
    * 데이터 양이 많을 때, 성능: LSTM > GRU
* a hidden node 구조: 단순 1개 layer 대신 3개 layer가 특별한 방식으로 서로의 정보를 주고 받는다

#### GRU 구조 
* 총 3개 layer - Gates 2개 (reset, update gate) 로 구성
* Gates $r, z$
    * reset gate (r): 이전 상태를 얼마나 반영할지
    * update gate (z): 이전 상태와 현재 상태를 얼마만큼의 비율로 반영할지
    * 모든 gate는 주로 sigmoid 함수 사용 (*sigmoid 함수값 출력범위: 0~1)
    * tanh 가 사용되는 부분도 있음
* (LSTM) Cell State + Hidden State -> (GRU) Hidden State
* (LSTM) forget gate + input gate -> (GRU) update gate
* (LSTM) output gate -> (GRU) 없음


### Code (PyTorch)
```python
"""
RNN/LSTM/GRU 각각의 cell은 모두 동일한 파라미터를 가진다.

Parameters
---
input_size: input의 feature dimension을 넣어주어야 한다. time step이 아니라 **feature dimension**
hidden_size: 내부에서 어떤 feature dimension으로 바꿔주고 싶은지를 넣어주면 된다.
num_layers: rnn/lstm/gru layer를 얼마나 쌓을지
bias: bias term을 둘 것인가 (Default: True)
batch_first: batch가 0번 dimension으로 오게 하려면 이거 설정! 난 이거 설정 가정하고 설명했다. (Default: False)
dropout: 가지치기 얼마나 할지, generalization 잘안되면 이걸 조정하면 된다.
bidirectional: 양방향으로 할지 말지 (bidirectional 하면 [forward, backword] 로 feature dimension 2배 됨)

Input
---
input dimension은 (Batch, Time_step, Feature dimension) 순. (batch_first=True)

Output
---
outputs: (output, (hidden or hidden,cell)) 의 tuple 형태 (LSTM만 cell state 존재) 대체로 첫번째 output만 사용 (task 고려하여 결정). 
-. output: (batch, time_step, hidden dimension) 순이다. 양방향일 경우 hidden_size*2 
    *. 참고: 일반적으로 사용되는 many-to-one model의 경우 마지막 hidden state를 이용해 output을 계산
-. hidden state: 모든 layer의 hidden state를 담고있다. (batch, num_layer*num_direction, hidden dimension)
-. cell state: 모든 layer의 cell state를 담고있다. (batch, num_layer*num_direction, hidden dimension) (LSTM만 존재)

"""

import torch.nn as nn
class SingleRNN(nn.Module):
    """ Many to One Model """
    def __init__(self, rnn_type, input_size, hidden_size, dropout=0, bidirectional=False):
        super(SingleRNN, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_direction = int(bidirectional) + 1
        # self.rnn = RNN(input_size, hidden_size, 1, dropout=dropout, batch_first=True, bidirectional=bidirectional)
        self.rnn = LSTM(input_size, hidden_size, 1, dropout=dropout, batch_first=True, bidirectional=bidirectional)
        # self.rnn = GRU(input_size, hidden_size, 1, dropout=dropout, batch_first=True, bidirectional=bidirectional)

    def forward(self, input):
        # input shape: batch, seq, dim
        output = input
        rnn_output, _ = self.rnn(output)
        return rnn_output
```


# RNN 기반 방법론
## Sequence to Sequence (Seq2Seq)
* Many to Many: Many to one (Encoder) + One to many (Decoder)
    * Many to one (Encoder) : input sequence (X = (x1, x2, ...) -> 단일 벡터 ( Context Vector ; encoder의 마지막 time step의 hidden state) 로 인코딩
    * One to many (Decoder) : 단일 입력 벡터 에서 output sequence (Y = (y1, y2, ...) 생성
* Language Model 예제 (출처. [부스트캠프 AI Tech] U-stage. 4-3)
    * ![image](https://user-images.githubusercontent.com/98376833/162370771-ebad80d2-a4b0-4c40-ae0d-70de99926d55.png)
    * Encoder: 
        * RNN 셀의 마지막 hidden state(Context Vector) 를 decoder의 입력으로 들어가게 된다.
        * ![image](https://user-images.githubusercontent.com/98376833/162372338-994382c2-9042-42bc-9588-5b4057fcc430.png) 
    * Decoder: 
        * Decoder 는 encoder가 넘겨준 context vector 를 RNN 셀의 첫번째 hidden state로 사용한다. 
        * ![image](https://user-images.githubusercontent.com/98376833/162372466-fddc9bbe-cf2c-49d4-9d61-18d32394cb08.png)
        * ![image](https://user-images.githubusercontent.com/98376833/162372580-80a91140-aa32-45d9-b6c2-5802780a8bd8.png)
        * Train 시, 학습과정에서의 decoder는 encoder로 부터 받은 context vector 와 실제 정답 상황인 <sos> je suis étudiant를 입력으로 주는 teacher forcing 기법을 통해 je suis étudiant <eos> 가 나오도록 학습한다.
        * Test 시, Test 과정에서는 context vector 와 <sos> 토큰 (Output sentence의 첫번째 토큰) 만을 입력으로 받아 다음에 등장할 확률이 높은 단어를 예측하여 그 결과를 다음 time step으로 넣어주고 <eos> 토큰이 다음 단어로 예측 될때까지 이 과정을 반복한다.
        
  
---

#### Reference
* Seq2Seq ([부스트캠프 AI Tech] U-stage. 4-3) https://velog.io/@kgh732/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-U-stage.-4-3
* Pytorch 공식 문서: https://pytorch.org/docs/stable/nn.html#recurrent-layers
* [PyTorch] 시계열 데이터를 위한 RNN/LSTM/GRU 사용법과 팁: https://sanghyu.tistory.com/52
* RNN vs LSTM vs GRU 구조 비교 ([DL] RNN, LSTM, GRU 란?) https://wooono.tistory.com/242
