---
layout: post
title:  "[DL] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (NAACL, 2019)"
excerpt: "Deep Learning - NLP - BERT"

categories:
  - ds
tags:
  - [DL] 

toc: true
toc_sticky: true
 
date: 2022-04-13
last_modified_at: 2022-04-13

---

# BERT 3줄 요약
* Transformer (2017, NIPS) 의 인코더를 활용한 모델이다.
* MLM (Masked Language Model) 과 NSP (Next Sentence Prediction) 방식을 사용하여 비지도학습 모델인 Pre-training BERT 를 학습시키고. 이 학습된 모델의 정보를 임베딩 정보로 활용할 수 있다. 
    * + 최종 Supervised Task 수행 시엔, 이 pre-trained 모델을 fine-tuning 하여 사용하면 된다 (전이학습).
* MLM은 입력 문장에서 임의 토큰을 마스킹 한 후, 마스킹된 단어를 예측하는 것이고 / NSP는 두 문장 (문장 pair)를 주고 문장의 순서 (어떤 문장이 뒤에 위치하는지) 를 예측한다.

## Summary
* BERT: Bidirectional Encoder Representations from Transformers
* 2018년 google 발표. 범용 언어 모델
* 제안된 모델 구조
    * Position Embeddings : Input representation 시 static position encoding (Transformer, 2017) 이 아닌 position embeddings 도입 (BERT, 2019)

---

# What is a BERT?
* BERT: Bidirectional Encoder Representations from Transformers
* Transformer (2017, NIPS) 의 인코더를 활용한 모델
* Semi-supervised Learning 수행
   * Pre-training BERT (Unsupervised Learning) : Large Corpus 로 부터 범용적인 모델 학습
   * Fine-tuning BERT (Supervised Learning) : pre-trained 모델 기반으로 각 task 에 적합한 모델 학습

## BERT - Model Architecture
![image](https://user-images.githubusercontent.com/98376833/163120526-c835632d-c707-4e35-aa8c-23162daf4967.png)

* **Transformer (2017, NIPS) 의 encoder 부분** 을 여러층으로 쌓은 구조
    * BERT_base: L=12개의 Transformer encoder block 사용 
    * BERT_large: L=24개의 Transformer encoder block 사용 

### Input/Output Representation
![image](https://user-images.githubusercontent.com/98376833/163121973-856f6aa8-15e7-45af-bd6c-c2ce96f6103d.png)

* *참고: Transformer (2017, NIPS): Static Positional Encoding 사용 (sin/cos 함수 활용)
* BERT (2019, NAACL) 에서는 **Position Embeddings** 사용 --> 학습 가능한 Embedding Layer 
* 1) The input embeddings are the sum of the token embeddings, the segment embeddings and the position embeddings.
   * Token Embeddings: 문장 (sentence) 를 token 단위로 구분
   * Segment Embeddings: 같이 처리해야할 pair 문장에서 첫번째 문장과 두번째 문장에 속하는 토큰을 구분
   * Position Embeddings: 단어의 순서를 구분
* 2) 1)의 3개 임베딩 합산 결과를 encoder 의 입력 값으로 전달


## Pre-training BERT
* 대량의 코퍼스로 언어 전반에 대한 이해하는 step
* 1) (Masked LR) 문장 내 단어를 랜덤하게 마스킹 한 후 마스킹된 단어를 예측 
* 2) OR (NSP) 두 문장 중에 두번째 문장이 첫번째 문장의 바로 다음에 오는 문장인지 여부 예측
* (BERT, 2019) activate function 으로 GELU 활용 (transformer, 2017 (NIPS) 는 ReLU 사용)
    * *참고: GELU 함수는 음수에 대해서도 미분 가능

### Task #1: Masked LM
* Use the output of the masked word's position to predict the masked word.

### Task #2: Next Sentence Prediction (NSP)
* Predict likelihood that sentence B belongs after sentence A. 
 
## Fine-tuning BERT
* 사전에 학습된 언어에 대한 폭넓은 이해를 바탕으로 특정 문제에 맞춰 적용하는 step
* Pre-trained 모델을 transfer 하여 Supervised Task 를 수행한다 (전이학습). 
    * 새로운 network를 사용할 필요 없이, BERT 모델 자체의 fine-tuning 을 통해 supervised learning 수행 

---

# Comparison of BERT, ELMo, and OpenAI GPT (in BERT paper)
![image](https://user-images.githubusercontent.com/98376833/163124327-a65080fc-c85e-4bd0-8a86-34ab51b53dc3.png)

## Pre-trained Language Representations 의 두가지 Strategies: a) Feature-based and b) Fine-tuning
### Feature-based approach (ELMo)
* pre-trained representations을 additionnal features 로 표현

#### ELMo
* ELMo uses the concatenation of independently trained left-to-right and right-to-left LSTMS to generate features for downstream tasks.

### Fine-tuning approach (GPT, BERT)
* The fine-tuning approach, introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.

#### BERT vs GPT
* BERT: uses a bidirectionl Transformer.
* OpenAI GPT: uses a left-to-right Transformer (uni-directional Transformer)


---

#### Reference LINK.
* (2019, NAACL) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 
