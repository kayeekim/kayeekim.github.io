---
layout: post
title:  "[DL] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (NAACL, 2019)"
excerpt: "Deep Learning - NLP - BERT"

categories:
  - ds
tags:
  - [DL] 

toc: true
toc_sticky: true
 
date: 2022-04-13
last_modified_at: 2022-04-13

---

## Summary
* BERT: Bidirectional Encoder Representations from Transformers
* 2018년 google 발표. 범용 언어 모델
* 제안된 모델 구조
    * Position Embeddings : Input representation 시 static position encoding (Transformer, 2017) 이 아닌 position embeddings 도입 (BERT, 2019)


# What is a BERT?
* BERT: Bidirectional Encoder Representations from Transformers
* Semi-supervised Learning 수행
   * Pre-training BERT (Unsupervised Learning) : Large Corpus 로 부터 범용적인 모델 학습
   * Fine-tuning BERT (Supervised Learning) : pre-trained 모델 기반으로 각 task 에 적합한 모델 학습

## BERT - Model Architecture
![image](https://user-images.githubusercontent.com/98376833/163120526-c835632d-c707-4e35-aa8c-23162daf4967.png)

* **Transformer (2017, NIPS) 의 encoder 부분** 을 여러층으로 쌓은 구조
    * BERT_base: L=12개의 Transformer encoder block 사용 
    * BERT_large: L=24개의 Transformer encoder block 사용 

### Input/Output Representation
![image](https://user-images.githubusercontent.com/98376833/163121973-856f6aa8-15e7-45af-bd6c-c2ce96f6103d.png)

* *참고: Transformer (2017, NIPS): Static Positional Encoding 사용 (sin/cos 함수 활용)
* BERT (2019, NAACL) 에서는 **Position Embeddings** 사용 --> 학습 가능한 Embedding Layer 
* 1) The input embeddings are the sum of the token embeddings, the segment embeddings and the position embeddings.
   * Token Embeddings: 문장 (sentence) 를 token 단위로 구분
   * Segment Embeddings: 같이 처리해야할 pair 문장에서 첫번째 문장과 두번째 문장에 속하는 토큰을 구분
   * Position Embeddings: 단어의 순서를 구분
* 2) 1)의 3개 임베딩 합산 결과를 encoder 의 입력 값으로 전달


## Pre-training BERT
* 대량의 코퍼스로 언어 전반에 대한 이해하는 step
* 1) (Masked LR) 문장 내 단어를 랜덤하게 마스킹 한 후 마스킹된 단어를 예측 
* 2) OR (NSP) 두 문장 중에 두번째 문장이 첫번째 문장의 바로 다음에 오는 문장인지 여부 예측
* (BERT, 2019) activate function 으로 GELU 활용 (transformer, 2017 (NIPS) 는 ReLU 사용)
    * *참고: GELU 함수는 음수에 대해서도 미분 가능

### Task #1: Masked LM

### Task #2: Next Sentence Prediction (NSP)
 
## Fine-tuning BERT
* 사전에 학습된 언어에 대한 폭넓은 이해를 바탕으로 특정 문제에 맞춰 적용하는 step




---

#### Reference LINK.
* (2019, NAACL) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 
